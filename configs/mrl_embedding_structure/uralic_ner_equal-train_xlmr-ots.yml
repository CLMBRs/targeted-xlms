# task setup
task: ner
zero_shot_transfer: False
transfer_source: None
langs: ['et', 'fi', 'fiu-vro', 'hu', 'koi', 'kv', 'mhr', 'mrj', 'myv', 'no', 'ru', 'vep']
# might sometimes need to break the list in half if there's risk of the node being pre-empted
dataset_path: /projects/assigned/txlm/data/ner-uralic
# whether to start with a randomly-initialized model
random_weights: False

# model options
model_class: xlmr
model_name: xlm-roberta-base
checkpoint_path: ./models/uralic/ner/equal_train/xlmr_ots

# training options
random_seed: 1
epochs: 64
batch_size: 36
gradient_accumulation: 2
max_train_examples: 1339
# checkpoints every two epochs, so patience epochs is this parameter x2
patience: 4
