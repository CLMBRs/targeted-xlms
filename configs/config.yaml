# datasets
train_dataset_path: "/Users/ibrahimsharafelden/workspace/TXLM/opus_data/data/georgian/CCMatrix_latest_mono_ka.txt"
val_dataset_path: "/Users/ibrahimsharafelden/workspace/TXLM/opus_data/data/georgian/CCMatrix_latest_mono_ka.txt"
dataset_block_size: 128
mlm_masking_prob: 0.15
max_len: 512

# training
seed: 42
hf_model: "xlm-roberta-large"
learning_rate: 5e-5
training_epochs: 1
train_batch_size: 64
eval_strategy: "steps" # no | steps | epoch
eval_steps: 10000
eval_batch_size: 64
checkpoints_directory: "./checkpoints"
save_steps: 10000
saved_checkpoints_limit: 5

# advanced training options
weight_decay: 0.0
lr_scheduler_type: "linear"
warmup_ratio: 0.0
warmup_steps: 0.0 # overrides warmup ratio
auto_find_batch_size: False # requires accelerate library
group_by_length: False # reduce padding effect
gradient_checkpointing: False
gradient_accumulation_steps: 1
fp16: False # mixed precision training
torch_distributed_training: False # fsdp parameter in HF code
full_determinism: False # ensure reproducible results in distributed training
