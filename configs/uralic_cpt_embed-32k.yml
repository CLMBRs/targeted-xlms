# datasets
train_dataset_path: "data/multilingual/uralic_train_alpha04_20mil"
sharded_train_dataset: True
val_dataset_path: "data/multilingual/uralic_dev_alpha04.txt"
mlm_masking_prob: 0.15
max_seq_len: 512

# training
seed: 1
hf_model: "xlm-roberta-base"
learning_rate: 1e-5
training_epochs: 2
train_batch_size: 50
gradient_accumulation_steps: 4
eval_strategy: "steps"
eval_steps: 5000
eval_batch_size: 128
checkpoints_directory: "models/uralic/cpt_embed_32k"
save_steps: 5000
saved_checkpoints_limit: 4

# advanced training options
new_vocab_file: tokenizers/uralic_spm_32k.model
freeze_main_model: True
model_freeze_prefix: "roberta.encoder"
weight_decay: 0.0
lr_scheduler_type: "linear"
warmup_ratio: 0.0
warmup_steps: 0.0 # overrides warmup ratio
auto_find_batch_size: False # requires accelerate library
group_by_length: False # reduce padding effect
gradient_checkpointing: False
fp16: False # mixed precision training
torch_distributed_training: False # fsdp parameter in HF code
full_determinism: False # ensure reproducible results in distributed training
